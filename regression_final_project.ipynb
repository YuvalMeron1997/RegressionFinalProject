{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Regression Methods Final Project"
   ],
   "metadata": {
    "id": "m_lTm5C6uNAv"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "itVHCevrG7c4"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "from numpy import linalg as LA\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oBDt-itpv155",
    "outputId": "1febdf54-7204-41ea-bf64-7cf9a8024ec2"
   },
   "execution_count": 70,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Linear Regression:"
   ],
   "metadata": {
    "id": "353F4Gg0ShES"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class LinearRegression1:\n",
    "    def __init__(self):\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_features = X.shape[1]\n",
    "        self.bias = 0\n",
    "        self.weights = np.zeros(n_features)\n",
    "        X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "        X = X.astype('float64')\n",
    "        y = y.astype('float64')\n",
    "        coefficients = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "        self.bias = coefficients[0]\n",
    "        self.weights = coefficients[1:]\n",
    "\n",
    "    def predict(self, X):\n",
    "        y_predicted = np.dot(X, self.weights) + self.bias\n",
    "        return y_predicted\n"
   ],
   "metadata": {
    "id": "B0EcO7hoHGIt"
   },
   "execution_count": 71,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Logistic Regression:"
   ],
   "metadata": {
    "id": "QoyKJ6cQSlRe"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class My_LogisticRegression:\n",
    "    def __init__(self, learning_rate=0.25, n_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_iterations = n_iterations\n",
    "        self.weights, self.bias = None, None\n",
    "\n",
    "    @staticmethod\n",
    "    def _sigmoid(x):\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    @staticmethod\n",
    "    def _binary_cross_entropy(y, y_hat):\n",
    "\n",
    "        def safe_log(x):\n",
    "            return 0 if x == 0 else np.log(x)\n",
    "\n",
    "        total = 0\n",
    "        for curr_y, curr_y_hat in zip(y, y_hat):\n",
    "            total += (curr_y * safe_log(curr_y_hat) + (1 - curr_y) * safe_log(1 - curr_y_hat))\n",
    "        return - total / len(y)\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X = X.astype(np.float64)\n",
    "        self.weights = np.zeros(X.shape[1])\n",
    "        self.bias = 0\n",
    "\n",
    "        for i in range(self.n_iterations):\n",
    "            linear_pred = np.dot(X, self.weights) + self.bias\n",
    "            probability = self._sigmoid(linear_pred)\n",
    "            # Calculate derivatives\n",
    "            partial_w = (1 / X.shape[0]) * (np.dot(X.T, (probability - y)))\n",
    "            partial_d = (1 / X.shape[0]) * (np.sum(probability - y))\n",
    "\n",
    "            self.weights -= self.learning_rate * partial_w\n",
    "            self.bias -= self.learning_rate * partial_d\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        X = X.astype(np.float64)\n",
    "        linear_pred = np.dot(X, self.weights) + self.bias\n",
    "        return self._sigmoid(linear_pred)\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        X = X.astype(np.float64)\n",
    "        probabilities = self.predict_proba(X)\n",
    "        return [1 if i > threshold else 0 for i in probabilities]\n"
   ],
   "metadata": {
    "id": "islcDYv9HJpB"
   },
   "execution_count": 72,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "PCA:"
   ],
   "metadata": {
    "id": "Jf_et5J_SprF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "class MY_PCA:\n",
    "    def __init__(self, k):\n",
    "        self.k = k\n",
    "        self.normalized_matrix = None\n",
    "        self.cov = None\n",
    "        self.values = None\n",
    "        self.vectors = None\n",
    "\n",
    "    def standartization(self, data):\n",
    "        mean = np.mean(data, axis=0)\n",
    "        std = np.std(data, axis=0,ddof=1)\n",
    "        self.normalized_matrix = (data-mean) / std\n",
    "\n",
    "    def cov_var_matrix(self):\n",
    "        cov_mat = np.cov(self.normalized_matrix, rowvar=False, bias=False)\n",
    "        self.cov = cov_mat\n",
    "\n",
    "    def eigen(self):\n",
    "        w, v = LA.eig(self.cov)\n",
    "        idx = w.argsort()[::-1]\n",
    "        w = w[idx]\n",
    "        v = v[:, idx]\n",
    "        selected_w = w.T[:self.k]\n",
    "        selected_V = v.T[:self.k]\n",
    "        self.values = selected_w\n",
    "        self.vectors = selected_V\n",
    "\n",
    "    def fit(self, data):\n",
    "        self.standartization(data)\n",
    "        self.cov_var_matrix()\n",
    "        self.eigen()\n",
    "\n",
    "    def transformation(self):\n",
    "        new_X = np.matmul(self.normalized_matrix, self.vectors.T)\n",
    "        return new_X\n",
    "\n"
   ],
   "metadata": {
    "id": "BXyataM3HR0Y"
   },
   "execution_count": 73,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Open the data:"
   ],
   "metadata": {
    "id": "01Iivi75SLyF"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def open_data(path):\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as file:\n",
    "        f = csv.reader(file)\n",
    "        data = np.array(list(f))\n",
    "        return data\n"
   ],
   "metadata": {
    "id": "4xaQQ-dmHVup"
   },
   "execution_count": 74,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data Pre-Processing:"
   ],
   "metadata": {
    "id": "cdD-52umSIbV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def clean_data(data, fill_nulls=None, test=0):\n",
    "    new_data = data.copy().astype(object)\n",
    "    if data.shape[1] > 53:  # only train has 53- \"expensive\"\n",
    "        column = data[1:, 53]\n",
    "        column[column == ''] = np.nan\n",
    "        # Replace \"f\" with 0 and \"t\" with 1\n",
    "        column = np.where(column == \"0\", 0, 1)\n",
    "        new_data[1:, 53] = column\n",
    "\n",
    "    # host_since 2\n",
    "    host_since = data[1:, 2]\n",
    "    host_since[host_since == ''] = np.nan\n",
    "    years = []\n",
    "    for date in host_since:\n",
    "        year = int(date.split('/')[-1])\n",
    "        years.append(2023 - year)\n",
    "    years_array = np.array(years)\n",
    "    new_data[1:, 2] = years_array\n",
    "    # 3 host_response_time\n",
    "    host_response_time = new_data[1:, 3]\n",
    "    mapping = {\n",
    "        'within an hour': 4,  # BETTER\n",
    "        'within a few hours': 3,\n",
    "        'within a day': 2,\n",
    "        'a few days or more': 1  # WORST\n",
    "    }\n",
    "    mapped_values = np.array([mapping.get(value, np.nan) for value in host_response_time])\n",
    "    new_data[1:, 3] = mapped_values\n",
    "    # host_response_rate 4\n",
    "    column = data[1:, 4]\n",
    "    column[column == ''] = np.nan\n",
    "    column_without_percent = []\n",
    "    for value in column:\n",
    "        if value == 'N/A':\n",
    "            column_without_percent.append(np.nan)\n",
    "        else:\n",
    "            column_without_percent.append(float(value.rstrip('%')) / 100)\n",
    "\n",
    "    # Calculate the average excluding NaN values\n",
    "    avg = np.nanmean(column_without_percent)\n",
    "\n",
    "    # Replace NaN values with the average\n",
    "    column_without_percent = np.where(np.isnan(column_without_percent), avg, column_without_percent)\n",
    "    column_without_percent1 = np.array(column_without_percent)\n",
    "    new_data[1:, 4] = column_without_percent1\n",
    "\n",
    "    # host_accaptence_rate 5\n",
    "    column = data[1:, 5]\n",
    "    column[column == ''] = np.nan\n",
    "    column_without_percent = []\n",
    "    for value in column:\n",
    "        if value == 'N/A':\n",
    "            column_without_percent.append(np.nan)\n",
    "        else:\n",
    "            column_without_percent.append(float(value.rstrip('%')) / 100)\n",
    "    # Calculate the average excluding NaN values\n",
    "    avg = np.nanmean(column_without_percent)\n",
    "    # Replace NaN values with the average\n",
    "    column_without_percent = np.where(np.isnan(column_without_percent), avg, column_without_percent)\n",
    "    column_without_percent1 = np.array(column_without_percent)\n",
    "    new_data[1:, 5] = column_without_percent1\n",
    "\n",
    "    # super_host 6\n",
    "    host_is_superhost = data[1:, 6]\n",
    "    column[column == ''] = np.nan\n",
    "    # Replace \"f\" with 0 and \"t\" with 1\n",
    "    host_is_superhost = np.where(host_is_superhost == \"f\", 0, 1)\n",
    "    new_data[1:, 6] = host_is_superhost\n",
    "\n",
    "    # 7\n",
    "    column = data[1:, 7]\n",
    "    column[column == ''] = np.nan\n",
    "    new_data[1:, 7] = column.astype(float)\n",
    "\n",
    "    # 8\n",
    "    column = data[1:, 8]\n",
    "    column[column == ''] = np.nan\n",
    "    new_data[1:, 8] = column.astype(float)\n",
    "\n",
    "    # 12\n",
    "    column = data[1:, 12]\n",
    "    column[column == ''] = np.nan\n",
    "    new_data[1:, 12] = column.astype(float)\n",
    "\n",
    "    # 13\n",
    "    column = data[1:, 13]\n",
    "    column[column == ''] = np.nan\n",
    "    new_data[1:, 13] = column.astype(float)\n",
    "\n",
    "    # 16\n",
    "    column = data[1:, 16]\n",
    "    column[column == ''] = np.nan\n",
    "    new_data[1:, 16] = column.astype(float)\n",
    "\n",
    "    # 18\n",
    "    column = new_data[1:, 18]\n",
    "    column[column == ''] = np.nan\n",
    "    new_data[1:, 18] = column.astype(float)\n",
    "\n",
    "    # 19\n",
    "    column = data[1:, 19]\n",
    "    column[column == ''] = np.nan\n",
    "    new_data[1:, 19] = column.astype(float)\n",
    "\n",
    "    for i in range(21, 29):\n",
    "        column = data[1:, i]\n",
    "        column[column == ''] = np.nan\n",
    "        new_data[1:, i] = column.astype(float)\n",
    "\n",
    "    for i in range(30, 37):\n",
    "        column = data[1:, i]\n",
    "        column[column == ''] = np.nan\n",
    "        new_data[1:, i] = column.astype(float)\n",
    "\n",
    "    for i in range(39, 46):\n",
    "        column = data[1:, i]\n",
    "        column[column == ''] = np.nan\n",
    "        new_data[1:, i] = column.astype(float)\n",
    "\n",
    "    for i in range(48, 53):\n",
    "        column = data[1:, i]\n",
    "        column[column == ''] = np.nan\n",
    "        new_data[1:, i] = column.astype(float)\n",
    "\n",
    "    # 10\n",
    "    column = data[1:, 10]\n",
    "    # Replace \"f\" with 0 and \"t\" with 1\n",
    "    column = np.where(column == \"f\", 0, 1)\n",
    "    new_data[1:, 10] = column\n",
    "    # 11\n",
    "    column = data[1:, 11]\n",
    "    # Replace \"f\" with 0 and \"t\" with 1\n",
    "    column = np.where(column == \"f\", 0, 1)\n",
    "    new_data[1:, 11] = column\n",
    "    # 29 has_availability\n",
    "    column = data[1:, 29]\n",
    "    # Replace \"f\" with 0 and \"t\" with 1\n",
    "    column = np.where(column == \"f\", 0, 1)\n",
    "    new_data[1:, 29] = column\n",
    "    # 47\n",
    "    instant = data[1:, 47]\n",
    "    # Replace \"f\" with 0 and \"t\" with 1\n",
    "    instant = np.where(instant == \"f\", 0, 1)\n",
    "    new_data[1:, 47] = instant\n",
    "\n",
    "    columns_to_drop = ['id', 'host_id','property_type','room_type',\n",
    "                       'host_verifications', 'bathrooms_text', 'amenities',\n",
    "                       'license', 'last_review',\n",
    "                       'first_review', \"host_identity_verified\",\n",
    "                       \"has_availability\", \"host_has_profile_pic\",\n",
    "                       'minimum_minimum_nights','maximum_maximum_nights',\n",
    "                       'minimum_maximum_nights','maximum_minimum_nights',\n",
    "                       'minimum_nights','availability_60',\n",
    "                       'availability_365','minimum_nights']\n",
    "    drop_indices = [i for i, title in enumerate(new_data[0]) if title in columns_to_drop]\n",
    "    new_data = np.delete(new_data, drop_indices, axis=1)\n",
    "\n",
    "  #filling nulls in train\n",
    "    if test == 0:\n",
    "        imputer = KNNImputer(n_neighbors=70)\n",
    "        new_data[1:] = imputer.fit_transform(new_data[1:])\n",
    "        column_means = new_data[1:,].mean(axis=0)\n",
    "\n",
    " # filling nulls in test by train values\n",
    "    if test == 1:\n",
    "        # Find the columns with null values\n",
    "        for i in range(0,new_data.shape[1]):\n",
    "            for j in range(1, new_data.shape[0]):\n",
    "                if np.isnan(new_data[j, i]):\n",
    "                  # mean from train of column i\n",
    "                    new_data[j, i] = fill_nulls[i]\n",
    "        column_means = fill_nulls\n",
    "\n",
    "    # review columns\n",
    "    column_names = new_data[0]\n",
    "    selected_columns = new_data[1:, np.isin(column_names, ['review_scores_rating',\n",
    "                                                           'review_scores_accuracy',\n",
    "                                                           'review_scores_cleanliness',\n",
    "                                                           'review_scores_checkin',\n",
    "                                                           'review_scores_communication',\n",
    "                                                           'review_scores_location',\n",
    "                                                           'review_scores_value'])]\n",
    "\n",
    "    #new column review - aggregation of the review columns\n",
    "    review_column = np.mean(selected_columns.astype(float), axis=1)\n",
    "    all_data_with_review = np.column_stack((new_data[1:, ], review_column))\n",
    "    # Add column names and 'review' to the array\n",
    "    titles = np.concatenate((column_names, ['review']))\n",
    "    new_data = np.vstack((titles, all_data_with_review))\n",
    "\n",
    "    columns_to_drop = ['review_scores_rating', 'review_scores_accuracy',\n",
    "                       'review_scores_cleanliness','review_scores_checkin',\n",
    "                       'review_scores_communication', 'review_scores_location',\n",
    "                       'review_scores_value']\n",
    "\n",
    "\n",
    "    drop_indices = [i for i, title in enumerate(new_data[0]) if title in columns_to_drop]\n",
    "    new_data = np.delete(new_data, drop_indices, axis=1)\n",
    "\n",
    "    if 'expensive' in new_data[0]:\n",
    "        expensive_column_index = np.where(new_data[0] == 'expensive')[0][0]\n",
    "        # Move the column to the last position\n",
    "        new_data[:, [expensive_column_index, -1]] = new_data[:, [-1, expensive_column_index]]\n",
    "\n",
    "\n",
    "    return new_data, column_means"
   ],
   "metadata": {
    "id": "C6GKKKp3HYCb"
   },
   "execution_count": 75,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Normalization:"
   ],
   "metadata": {
    "id": "SSHpSugxSPVN"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Min-Max on train data"
   ],
   "metadata": {
    "id": "2J2bgp75KgdK"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def minmax_on_train(data):\n",
    "    min_max_values = []\n",
    "    data_without_first_row = data[1:]\n",
    "    for i, column in enumerate(data_without_first_row.T):\n",
    "        min_value = np.min(column)\n",
    "        max_value = np.max(column)\n",
    "        min_max_values.append((min_value, max_value))\n",
    "        for j, value in enumerate(column):\n",
    "            scaled_value = (value - min_value) / (max_value - min_value)\n",
    "            data_without_first_row[j, i] = scaled_value\n",
    "    data_after_min_max = np.vstack((data[0], data_without_first_row))\n",
    "    return data_after_min_max, min_max_values\n",
    "\n"
   ],
   "metadata": {
    "id": "zRncvdYAHhwz"
   },
   "execution_count": 76,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Min-Max on test data:"
   ],
   "metadata": {
    "id": "27DF_a6DKnvo"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The function will normalized the test data based on the min-max values of the train."
   ],
   "metadata": {
    "id": "zYKjkfE2KwEM"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The second argument is the an array with tuples of (min,max) value of each column in the train"
   ],
   "metadata": {
    "id": "zPLcSm25K8IT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def min_max_on_test(data, min_max_by_train):\n",
    "    test_min_max = data[1:]\n",
    "    for i, column in enumerate(test_min_max.T):\n",
    "        min_value = np.min(min_max_by_train[i][0])\n",
    "        max_value = np.max(min_max_by_train[i][1])\n",
    "        for j, value in enumerate(column):\n",
    "            scaled_value = (value - min_value) / (max_value - min_value)\n",
    "            test_min_max[j, i] = scaled_value\n",
    "    test_min_max = np.vstack((data[0], test_min_max))\n",
    "    return test_min_max\n"
   ],
   "metadata": {
    "id": "SaBXbHCkOsAO"
   },
   "execution_count": 77,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Fitting the model on Training set and predicting on Testing set"
   ],
   "metadata": {
    "id": "IurusMk-SS0k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "data_train = open_data(\"/content/sample_data/train.csv\")\n",
    "data_test = open_data(\"/content/sample_data/test.csv\")"
   ],
   "metadata": {
    "id": "TEJWjV26Hk8B"
   },
   "execution_count": 78,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "clean_train = (clean_data(data_train))[0]\n",
    "columns_means_train = (clean_data(data_train))[1]\n",
    "clean_test = (clean_data(data_test, columns_means_train, 1))[0]\n",
    "X_train = clean_train[:, :-1]  # Input features (excluding the last column)\n",
    "y_train = clean_train[0:, -1]  # Target variable (last column)\n",
    "X_test = clean_test  # the test is only the features\n"
   ],
   "metadata": {
    "id": "bKo67biZO7ue"
   },
   "execution_count": 79,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "min_max_train = minmax_on_train(X_train)\n",
    "min_max_train_data = min_max_train[0]# train after min max\n",
    "min_max_train_values = min_max_train[1]# the min,max for each column by train\n",
    "min_max_test = min_max_on_test(X_test,min_max_train_values)"
   ],
   "metadata": {
    "id": "bEnlDXuQPAVk"
   },
   "execution_count": 80,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "model_rl = My_LogisticRegression()\n",
    "y_train1 = y_train[1:, ].astype(np.float64) #class\n",
    "model_rl.fit(min_max_train_data[1:, ], y_train1) #fit\n",
    "y_pred_model_rl = model_rl.predict(min_max_test[1:, ]) #predict class\n",
    "y_prob = model_rl.predict_proba(min_max_test[1:, ]) #predict probability\n",
    "print(\"my predicted probabilities:\")\n",
    "print(y_prob)\n",
    "y_pred = pd.Series(y_prob, name='prediction')\n",
    "y_pred.to_csv('prediction.csv', index=False)\n"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eNRXc_s_P2iv",
    "outputId": "4fd3ef72-133a-4a81-94aa-39cbd804248a"
   },
   "execution_count": 81,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "my predicted probabilities:\n",
      "[0.77816021 0.81601169 0.89502025 ... 0.9621787  0.86932429 0.86435212]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "comaprison to sklearn models:\n"
   ],
   "metadata": {
    "id": "ENTyqybNYlwJ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "pca =MY_PCA(5)\n",
    "d = min_max_train_data[1:, ].astype(float)\n",
    "pca.fit(d)\n",
    "data_after_pca = pca.transformation()\n",
    "print(data_after_pca)\n",
    "\n",
    "print(\"------------------\")\n",
    "\n",
    "sk_pca = PCA(5,random_state=2023)\n",
    "d = min_max_train_data[1:, ].astype(float)\n",
    "data_after_pca_sk = sk_pca.fit_transform(d)\n",
    "print(data_after_pca_sk)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sT0s_dz4Sf7x",
    "outputId": "e6ae7b4b-2caf-4af4-c9cb-006b4f54b4c7"
   },
   "execution_count": 82,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[-1.32070006  1.05589145  0.95841549 -1.15955979  1.95113887]\n",
      " [-1.72217631 -0.31214451  2.04296912 -0.0661073   0.1101341 ]\n",
      " [-2.80703065 -1.18509898  2.72961886 -0.39761819  1.12719859]\n",
      " ...\n",
      " [ 2.64115865 -4.7352117  -0.95374057  0.15569734  1.19182884]\n",
      " [ 3.73319598 -4.06974013 -0.36720657 -0.4785281   0.71064052]\n",
      " [ 1.6142069   0.02003486  0.91944912  2.16315359 -1.21317153]]\n",
      "------------------\n",
      "[[ 0.87173865 -0.16813202 -0.29813838  0.02426754 -0.1342138 ]\n",
      " [-0.4366185  -0.34751394  0.28557803 -0.30819223 -0.14345212]\n",
      " [ 0.4802321  -0.32342062  0.46861815 -0.54151679 -0.06952668]\n",
      " ...\n",
      " [-0.39457014  0.57414193  0.36848856  0.33054134  0.69087735]\n",
      " [-0.39560035  0.54811973  0.39279652  0.33194863  0.74979791]\n",
      " [-0.4815363  -0.09370243  0.95385107  0.50385257  0.14254565]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lr =LinearRegression1()\n",
    "d = min_max_train_data[1:, ]\n",
    "lr.fit(d,y_train1)\n",
    "prediction = lr.predict(min_max_test[1:,])\n",
    "print(prediction)\n",
    "\n",
    "print(\"------------------\")\n",
    "\n",
    "lr_sk =LinearRegression()\n",
    "d = min_max_train_data[1:, ]\n",
    "lr_sk.fit(d,y_train1)\n",
    "prediction = lr_sk.predict(min_max_test[1:,])\n",
    "print(prediction)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lHCOLbghW-Jy",
    "outputId": "3ae31447-1f1f-46ff-dcd6-34b3d1c9540b"
   },
   "execution_count": 83,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.7772387094757583 0.9364974440450813 0.9615421270580387 ...\n",
      " 1.1630503623231918 0.9319726009077961 0.9266228554437942]\n",
      "------------------\n",
      "[0.77723871 0.93649744 0.96154213 ... 1.16305036 0.9319726  0.92662286]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "lr =My_LogisticRegression()\n",
    "d = min_max_train_data[1:, ]\n",
    "lr.fit(d,y_train1)\n",
    "prediction = lr.predict(min_max_test[1:,])\n",
    "probs = lr.predict_proba(min_max_test[1:,])\n",
    "print(probs)\n",
    "\n",
    "print(\"------------------\")\n",
    "\n",
    "lr_sk =LogisticRegression(max_iter=1000,random_state=2023)\n",
    "d = min_max_train_data[1:, ]\n",
    "lr_sk.fit(d,y_train1)\n",
    "prediction = lr_sk.predict(min_max_test[1:,])\n",
    "probs = lr_sk.predict_proba(min_max_test[1:,])\n",
    "print(probs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LjshB861YsQR",
    "outputId": "83d8dd80-e178-4811-b9ec-96688d08b58c"
   },
   "execution_count": 84,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.77816021 0.81601169 0.89502025 ... 0.9621787  0.86932429 0.86435212]\n",
      "------------------\n",
      "[[0.16074484 0.83925516]\n",
      " [0.04021555 0.95978445]\n",
      " [0.0834169  0.9165831 ]\n",
      " ...\n",
      " [0.00731717 0.99268283]\n",
      " [0.10291227 0.89708773]\n",
      " [0.10710063 0.89289937]]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#predictions:\n",
    "print(y_prob)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dpnqg8Ks0Jrn",
    "outputId": "6f880ba2-65ec-4034-d307-5058c9037496"
   },
   "execution_count": 85,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.77816021 0.81601169 0.89502025 ... 0.9621787  0.86932429 0.86435212]\n"
     ]
    }
   ]
  }
 ]
}
